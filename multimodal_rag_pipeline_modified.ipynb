{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "36efbd27",
   "metadata": {},
   "source": [
    "# Multimodal RAG (PDF → Text/Images/Tables → Cohere Embeddings → FAISS → Azure OpenAI Q&A)\n",
    "\n",
    "This notebook implements a production-ready, step-by-step pipeline for *multimodal RAG* on PDFs:\n",
    "\n",
    "1. **Extraction**: Separate functions for **text**, **images**, and **tables**.\n",
    "2. **Heading-aware chunking**: Text is chunked by headings (each heading and its following body is one section).\n",
    "3. **Embeddings**: Cohere embeddings for **text**, **tables** (Markdown), and **images** (data URI).\n",
    "4. **Vector store**: Store all embeddings in FAISS with rich metadata.\n",
    "5. **RAG query**: Use **Azure OpenAI** (via `AzureChatOpenAI`) to answer questions grounded in retrieved chunks.\n",
    "\n",
    "At each stage, the notebook surfaces JSON records like:\n",
    "```json\n",
    "{\n",
    "  \"type\": \"table\",\n",
    "  \"title\": \"Annual Sales Data\",\n",
    "  \"embedding\": [0.2, 0.3, 0.5],\n",
    "  \"metadata\": {\"source\": \"Report2025.pdf\", \"page\": 12}\n",
    "}\n",
    "```\n",
    "\n",
    "**Notes & Accuracy Choices**\n",
    "- **Text / Headings**: Uses PyMuPDF's font-size & span inspection to detect headings robustly.\n",
    "- **Images**: Uses PyMuPDF's `get_images()` / `extract_image()` + nearby caption detection (e.g., *Figure 1:*).\n",
    "- **Tables**: Uses Camelot (both `lattice` and `stream`, with fallbacks) and pdfplumber as the final fallback. Captions are resolved heuristically from nearby lines (e.g., *Table 1:*).\n",
    "- **Cohere embeddings**: A single model for all modalities (text + images) so vector dimensions match. Suggested default: `embed-multilingual-v3.0` (1024 dims). Changeable in config.\n",
    "- **Vector store**: FAISS (cosine similarity) with normalized vectors.\n",
    "- **LLM**: Azure OpenAI chat model via LangChain (your deployment name in `LLM_MODEL`).\n",
    "\n",
    "Sections are function-based and callable individually so you can **verify results step-by-step**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "03d5f9e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%capture\n",
    "# If running first time, uncomment installs below:\n",
    "# !pip install -U pymupdf camelot-py[cv] pdfplumber cohere langchain langchain-community langchain-openai faiss-cpu pandas numpy pillow\n",
    "# Optional high-res PDF parsing (heavier deps):\n",
    "# !pip install unstructured unstructured-inference\n",
    "\n",
    "import os, re, io, base64, math, tempfile, json, textwrap\n",
    "from typing import List, Dict, Any, Tuple\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import fitz  # PyMuPDF\n",
    "import pdfplumber\n",
    "import camelot\n",
    "import cohere\n",
    "from PIL import Image\n",
    "\n",
    "# Vector store\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.docstore.in_memory import InMemoryDocstore\n",
    "from langchain_core.documents import Document\n",
    "from langchain_openai import AzureChatOpenAI\n",
    "import openai\n",
    "\n",
    "try:\n",
    "    import faiss  # noqa\n",
    "except Exception as e:\n",
    "    raise RuntimeError(\"FAISS import failed. Try installing faiss-cpu: pip install faiss-cpu\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1a26c97",
   "metadata": {},
   "source": [
    "## Config\n",
    "Set your API keys and model names. We assume Azure OpenAI credentials are set in env vars.\n",
    "\n",
    "- `COHERE_API_KEY`  \n",
    "- `AZURE_OPENAI_TYPE`, `AZURE_OPENAI_ENDPOINT`, `AZURE_OPENAI_API_KEY`, `AZURE_OPENAI_API_VERSION`  \n",
    "- `LLM_MODEL` (your Azure OpenAI **deployment** name)  \n",
    "\n",
    "You can also pick a Cohere embeddings model. For **multimodal** (text + images) use a v3+ model, e.g. `embed-multilingual-v3.0`. For v4, set `EMBED_OUTPUT_DIM` if you want non-default output dims."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ea9d411",
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Set COHERE_API_KEY",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAssertionError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[22]\u001b[39m\u001b[32m, line 12\u001b[39m\n\u001b[32m      9\u001b[39m openai.api_version = os.getenv(\u001b[33m\"\u001b[39m\u001b[33mAZURE_OPENAI_API_VERSION\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m2024-10-21\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     10\u001b[39m LLM_MODEL          = os.getenv(\u001b[33m\"\u001b[39m\u001b[33mLLM_MODEL\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mgpt-4o-mini\u001b[39m\u001b[33m\"\u001b[39m)  \u001b[38;5;66;03m# your Azure deployment name\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m COHERE_API_KEY, \u001b[33m\"\u001b[39m\u001b[33mSet COHERE_API_KEY\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m openai.api_key \u001b[38;5;129;01mand\u001b[39;00m openai.api_base \u001b[38;5;129;01mand\u001b[39;00m openai.api_version, \u001b[33m\"\u001b[39m\u001b[33mSet Azure OpenAI env vars\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     15\u001b[39m co = cohere.ClientV2(api_key=COHERE_API_KEY)\n",
      "\u001b[31mAssertionError\u001b[39m: Set COHERE_API_KEY"
     ]
    }
   ],
   "source": [
    "# === ENV / Model Config ===\n",
    "COHERE_API_KEY = os.getenv(\"COHERE_API_KEY\", \"\")\n",
    "EMBED_MODEL    = os.getenv(\"EMBED_MODEL\", )  # supports text + image, 1024 dims\n",
    "EMBED_OUTPUT_DIM = None  # for embed-v4.0 you can set 256/512/1024/1536; leave None for model default\n",
    "\n",
    "openai.api_type    = os.getenv(\"AZURE_OPENAI_TYPE\", \"\")\n",
    "openai.api_base    = os.getenv(\"AZURE_OPENAI_ENDPOINT\", \"\")\n",
    "openai.api_key     = os.getenv(\"AZURE_OPENAI_API_KEY\", \"\")\n",
    "openai.api_version = os.getenv(\"AZURE_OPENAI_API_VERSION\",\"\")\n",
    "LLM_MODEL          = os.getenv(\"LLM_MODEL\",\"\")  # your Azure deployment name\n",
    "\n",
    "assert COHERE_API_KEY, \"Set COHERE_API_KEY\"\n",
    "assert openai.api_key and openai.api_base and openai.api_version, \"Set Azure OpenAI env vars\"\n",
    "\n",
    "co = cohere.ClientV2(api_key=COHERE_API_KEY)\n",
    "llm = AzureChatOpenAI(\n",
    "    azure_deployment=LLM_MODEL,\n",
    "    openai_api_key=openai.api_key,\n",
    "    openai_api_version=openai.api_version,\n",
    "    azure_endpoint=openai.api_base,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55d5bc27",
   "metadata": {},
   "source": [
    "## Utilities\n",
    "Helper functions for cleaning, caption search, and data-URI conversion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dd56a8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _clean_text(s: str) -> str:\n",
    "    s = s.replace('\\u00ad', '')  # soft hyphen\n",
    "    s = re.sub(r\"\\s+\", \" \", s)\n",
    "    return s.strip()\n",
    "\n",
    "def _to_data_uri(image_bytes: bytes, ext: str) -> str:\n",
    "    fmt = \"jpeg\" if ext.lower() in [\"jpg\", \"jpeg\"] else \"png\"\n",
    "    b64 = base64.b64encode(image_bytes).decode(\"utf-8\")\n",
    "    return f\"data:image/{fmt};base64,{b64}\"\n",
    "\n",
    "def _table_to_markdown(df: pd.DataFrame) -> str:\n",
    "    # Render as pipe-table markdown for embedding & readability\n",
    "    return df.to_markdown(index=False)\n",
    "\n",
    "def _cosine_normalize(mat: np.ndarray) -> np.ndarray:\n",
    "    norms = np.linalg.norm(mat, axis=1, keepdims=True) + 1e-12\n",
    "    return mat / norms\n",
    "\n",
    "def _guess_caption_from_lines(lines: List[Tuple[float, float, str]], pivot_y: float, kind: str = \"figure\") -> str:\n",
    "    \"\"\"Find the nearest preceding line that looks like a caption.\n",
    "    lines: list of (y0, y1, text) sorted top→bottom\n",
    "    pivot_y: search above this y (top is small y)\n",
    "    kind: 'figure' or 'table'\n",
    "    \"\"\"\n",
    "    pattern = r\"^(?:%s|%s)\\s*\\d+\\s*[:\\.-]?\\s*(.+)$\" % (kind.capitalize(), kind[:3].capitalize()+r\"\\.?\")\n",
    "    candidates = [t for (y0, y1, t) in lines if y1 <= pivot_y + 2]  # lines above or touching pivot\n",
    "    for txt in reversed(candidates[-10:]):  # look in last 10 lines above\n",
    "        m = re.match(pattern, txt.strip())\n",
    "        if m:\n",
    "            return _clean_text(m.group(0))\n",
    "    # fallback: return empty\n",
    "    return \"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b14fa2ee",
   "metadata": {},
   "source": [
    "## Text Extraction & Heading-Aware Chunking (PyMuPDF)\n",
    "This uses `page.get_text(\"dict\")` to inspect spans (font sizes, flags) and identify headings by **size percentile** + **bold/uppercase** heuristics, then groups the text between headings into sections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a8951112",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Improved text extraction function with hierarchical heading detection\n",
    "def extract_text_sections_by_headings(pdf_path: str, skip_contents: bool = True, fallback_page_chunk: bool = True) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Extract hierarchical text sections by recognizing headings and subheadings.\n",
    "    The section title is built as 'Level1 heading -> Level2 heading'.\n",
    "    Lines with heading levels deeper than 2 are treated as part of the body.\n",
    "\n",
    "    Args:\n",
    "        pdf_path: Path to a PDF file.\n",
    "        skip_contents: If True, skip pages that begin with 'Contents' or 'Table of Contents'.\n",
    "        fallback_page_chunk: If True and no headings are found, chunk by page.\n",
    "\n",
    "    Returns:\n",
    "        A list of section dictionaries with type, title, content, and metadata.\n",
    "    \"\"\"\n",
    "    import fitz\n",
    "    import re\n",
    "    doc = fitz.open(pdf_path)\n",
    "    sections: List[Dict[str, Any]] = []\n",
    "    current_h1 = None\n",
    "    current_h2 = None\n",
    "    current_lines: List[str] = []\n",
    "    section_start_page = 0\n",
    "    headings_found = 0\n",
    "\n",
    "    def flush_section(end_page: int):\n",
    "        nonlocal current_lines, current_h1, current_h2, section_start_page\n",
    "        if not current_lines:\n",
    "            return\n",
    "        text = ' '.join(current_lines).strip()\n",
    "        if not text:\n",
    "            current_lines.clear()\n",
    "            return\n",
    "        if current_h2:\n",
    "            title = f\"{current_h1} -> {current_h2}\"\n",
    "        elif current_h1:\n",
    "            title = current_h1\n",
    "        else:\n",
    "            title = \"Untitled Section\"\n",
    "        sections.append({\n",
    "            'type': 'text',\n",
    "            'title': title,\n",
    "            'content': _clean_text(text),\n",
    "            'metadata': {\n",
    "                'page_start': section_start_page,\n",
    "                'page_end': end_page\n",
    "            }\n",
    "        })\n",
    "        current_lines.clear()\n",
    "\n",
    "    for pno in range(len(doc)):\n",
    "        page = doc[pno]\n",
    "        # Skip contents page if the first line indicates a table of contents\n",
    "        if skip_contents:\n",
    "            first_line = page.get_text(\"text\").strip().split(\" \", 1)[0].strip().lower()\n",
    "            if re.match(r'(table of contents|contents)', first_line):\n",
    "                continue\n",
    "        lines = page.get_text(\"text\").split(\" \")\n",
    "        for line in lines:\n",
    "            line_stripped = line.strip()\n",
    "            if not line_stripped:\n",
    "                continue\n",
    "            # Numeric heading detection (e.g., '1 Introduction', '1.1 Purpose')\n",
    "            num_match = re.match(r'^(\\d+(?:\\.\\d+)*)\\s+(.+)$', line_stripped)\n",
    "            if num_match:\n",
    "                number = num_match.group(1)\n",
    "                heading_text = num_match.group(2).strip()\n",
    "                level = number.count('.') + 1\n",
    "                if level == 1:\n",
    "                    flush_section(pno)\n",
    "                    current_h1 = heading_text\n",
    "                    current_h2 = None\n",
    "                    section_start_page = pno\n",
    "                    headings_found += 1\n",
    "                    continue\n",
    "                elif level == 2:\n",
    "                    flush_section(pno)\n",
    "                    if current_h1 is None:\n",
    "                        current_h1 = heading_text\n",
    "                        current_h2 = None\n",
    "                    else:\n",
    "                        current_h2 = heading_text\n",
    "                    section_start_page = pno\n",
    "                    headings_found += 1\n",
    "                    continue\n",
    "                else:\n",
    "                    # Levels deeper than 2 are treated as normal text\n",
    "                    pass\n",
    "            # All-caps heading fallback (short uppercase phrases without numbers)\n",
    "            cap_match = re.match(r'^[A-Z][A-Z\\s]{2,}$', line_stripped)\n",
    "            if cap_match and len(line_stripped.split()) <= 10:\n",
    "                flush_section(pno)\n",
    "                if current_h1 is None:\n",
    "                    current_h1 = line_stripped.title()\n",
    "                    current_h2 = None\n",
    "                elif current_h2 is None:\n",
    "                    current_h2 = line_stripped.title()\n",
    "                else:\n",
    "                    current_h2 = line_stripped.title()\n",
    "                section_start_page = pno\n",
    "                headings_found += 1\n",
    "                continue\n",
    "            # Otherwise, treat as body text\n",
    "            current_lines.append(line_stripped)\n",
    "        # End of line processing for page\n",
    "    # Flush the final accumulated lines\n",
    "    flush_section(len(doc) - 1)\n",
    "    # Fallback: If no headings were detected, chunk by page\n",
    "    if headings_found == 0 and fallback_page_chunk:\n",
    "        sections = []\n",
    "        for pno in range(len(doc)):\n",
    "            page_text = doc[pno].get_text(\"text\").strip()\n",
    "            if not page_text:\n",
    "                continue\n",
    "            sections.append({\n",
    "                'type': 'text',\n",
    "                'title': f\"Page {pno + 1}\",\n",
    "                'content': _clean_text(page_text),\n",
    "                'metadata': {'page_start': pno, 'page_end': pno}\n",
    "            })\n",
    "    return sections\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75ca5837",
   "metadata": {},
   "source": [
    "## Image Extraction (PyMuPDF)\n",
    "Extracts images via `page.get_images()` / `doc.extract_image(xref)` and attempts to find a nearby caption (e.g., `Figure 1:`) from the page text. Returns JSON-like records with paths and metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2156d9b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_images(pdf_path: str, save_dir: str = \"extracted_images\") -> List[Dict[str, Any]]:\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    doc = fitz.open(pdf_path)\n",
    "    out = []\n",
    "    for pno in range(len(doc)):\n",
    "        page = doc[pno]\n",
    "        # Prepare text lines for caption heuristics\n",
    "        d = page.get_text(\"dict\")\n",
    "        lines = []\n",
    "        for b in d.get('blocks', []):\n",
    "            for l in b.get('lines', []):\n",
    "                y0 = min(s.get('bbox', l.get('bbox', b.get('bbox')))[1] for s in l.get('spans', [])) if l.get('spans') else l.get('bbox', [0,0,0,0])[1]\n",
    "                y1 = max(s.get('bbox', l.get('bbox', b.get('bbox')))[3] for s in l.get('spans', [])) if l.get('spans') else l.get('bbox', [0,0,0,0])[3]\n",
    "                txt = _clean_text(\" \".join(s.get('text','') for s in l.get('spans', [])))\n",
    "                if txt:\n",
    "                    lines.append((y0, y1, txt))\n",
    "        lines.sort(key=lambda x: x[0])\n",
    "\n",
    "        for img in page.get_images(full=True):\n",
    "            xref = img[0]\n",
    "            try:\n",
    "                info = doc.extract_image(xref)\n",
    "            except Exception:\n",
    "                continue\n",
    "            if not info:\n",
    "                continue\n",
    "            ext = info.get('ext', 'png')\n",
    "            img_bytes = info.get('image')\n",
    "            # save to disk for inspection\n",
    "            img_name = f\"page{pno+1}_xref{xref}.{ext}\"\n",
    "            img_path = os.path.join(save_dir, img_name)\n",
    "            with open(img_path, 'wb') as f:\n",
    "                f.write(img_bytes)\n",
    "            # estimate vertical position using image rects (if available)\n",
    "            rects = page.get_image_rects(xref)\n",
    "            pivot_y = rects[0].y1 if rects else page.rect.height/2\n",
    "            caption = _guess_caption_from_lines(lines, pivot_y, kind=\"figure\")\n",
    "            title = caption if caption else f\"Image (Page {pno+1})\"\n",
    "            out.append({\n",
    "                'type': 'image',\n",
    "                'title': title,\n",
    "                'image_path': img_path,\n",
    "                'metadata': {'source': os.path.basename(pdf_path), 'page': pno+1}\n",
    "            })\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de36103b",
   "metadata": {},
   "source": [
    "## Table Extraction (Camelot → pdfplumber fallback)\n",
    "Tries Camelot `lattice` and `stream` per page; if nothing is detected, uses pdfplumber as a fallback. Also attempts to find **table captions** by scanning nearby lines for patterns like `Table 1:`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "dcca932d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_tables(pdf_path: str) -> List[Dict[str, Any]]:\n",
    "    doc = fitz.open(pdf_path)\n",
    "    results = []\n",
    "    # Build page lines for caption search\n",
    "    page_lines = []\n",
    "    for pno in range(len(doc)):\n",
    "        page = doc[pno]\n",
    "        d = page.get_text(\"dict\")\n",
    "        lines = []\n",
    "        for b in d.get('blocks', []):\n",
    "            for l in b.get('lines', []):\n",
    "                y0 = min(s.get('bbox', l.get('bbox', b.get('bbox')))[1] for s in l.get('spans', [])) if l.get('spans') else l.get('bbox', [0,0,0,0])[1]\n",
    "                y1 = max(s.get('bbox', l.get('bbox', b.get('bbox')))[3] for s in l.get('spans', [])) if l.get('spans') else l.get('bbox', [0,0,0,0])[3]\n",
    "                txt = _clean_text(\" \".join(s.get('text','') for s in l.get('spans', [])))\n",
    "                if txt:\n",
    "                    lines.append((y0, y1, txt))\n",
    "        lines.sort(key=lambda x: x[0])\n",
    "        page_lines.append(lines)\n",
    "\n",
    "    # Try Camelot first (lattice then stream)\n",
    "    try:\n",
    "        tables_lattice = camelot.read_pdf(pdf_path, pages='all', flavor='lattice')\n",
    "    except Exception:\n",
    "        tables_lattice = None\n",
    "    try:\n",
    "        tables_stream = camelot.read_pdf(pdf_path, pages='all', flavor='stream')\n",
    "    except Exception:\n",
    "        tables_stream = None\n",
    "\n",
    "    # Merge both\n",
    "    camelot_tables = []\n",
    "    for coll in [tables_lattice, tables_stream]:\n",
    "        if coll is not None and coll.n > 0:\n",
    "            camelot_tables.extend(list(coll))\n",
    "\n",
    "    # If Camelot found none, fallback to pdfplumber\n",
    "    if not camelot_tables:\n",
    "        with pdfplumber.open(pdf_path) as pdf:\n",
    "            for pno, page in enumerate(pdf.pages):\n",
    "                try:\n",
    "                    tbls = page.extract_tables()\n",
    "                except Exception:\n",
    "                    tbls = []\n",
    "                for idx, data in enumerate(tbls or []):\n",
    "                    df = pd.DataFrame(data[1:], columns=data[0]) if data and data[0] else pd.DataFrame(data)\n",
    "                    title = _guess_caption_from_lines(page_lines[pno], pivot_y=page.height/2, kind=\"table\") or f\"Table (Page {pno+1})\"\n",
    "                    results.append({\n",
    "                        'type': 'table',\n",
    "                        'title': title,\n",
    "                        'dataframe': df,\n",
    "                        'metadata': {'source': os.path.basename(pdf_path), 'page': pno+1}\n",
    "                    })\n",
    "        return results\n",
    "\n",
    "    # Build results from Camelot\n",
    "    # Camelot tables have .df and .parsing_report with page info; order is original reading order\n",
    "    for i, tbl in enumerate(camelot_tables, start=1):\n",
    "        try:\n",
    "            df = tbl.df\n",
    "        except Exception:\n",
    "            # Skip broken tables\n",
    "            continue\n",
    "        # page number from parsing_report or fallback to 1\n",
    "        pg = int(tbl.parsing_report.get('page', 1)) if hasattr(tbl, 'parsing_report') else 1\n",
    "        # Heuristic caption: search above table middle if table bbox is available\n",
    "        try:\n",
    "            # tbl._bbox is non-public; if not present, use mid-page as pivot\n",
    "            bbox = getattr(tbl, '_bbox', None)\n",
    "            pivot_y = (bbox[3] if bbox else doc[pg-1].rect.height/2)\n",
    "        except Exception:\n",
    "            pivot_y = doc[pg-1].rect.height/2\n",
    "        title = _guess_caption_from_lines(page_lines[pg-1], pivot_y, kind=\"table\") or f\"Table (Page {pg})\"\n",
    "        results.append({\n",
    "            'type': 'table',\n",
    "            'title': title,\n",
    "            'dataframe': df,\n",
    "            'metadata': {'source': os.path.basename(pdf_path), 'page': pg}\n",
    "        })\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26e39752",
   "metadata": {},
   "source": [
    "## Embeddings with Cohere (text + images)\n",
    "Embeds:\n",
    "- **Text** chunks (sections) and **tables** (as Markdown) using `input_type='search_document'`.\n",
    "- **Images** by passing a **data URI** and `input_type='image'`.\n",
    "\n",
    "All embeddings are stored back into the item dict under `embedding`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67c3c0c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_items_with_cohere(items: List[Dict[str, Any]], co: cohere.ClientV2, model: str = EMBED_MODEL, output_dim: int = EMBED_OUTPUT_DIM) -> List[Dict[str, Any]]:\n",
    "    texts = []\n",
    "    text_idx = []\n",
    "    images = []\n",
    "    image_idx = []\n",
    "\n",
    "    # Prepare payloads\n",
    "    for i, it in enumerate(items):\n",
    "        typ = it['type']\n",
    "        if typ == 'text':\n",
    "            texts.append(it['content'])\n",
    "            text_idx.append(i)\n",
    "        elif typ == 'table':\n",
    "            md = _table_to_markdown(it['dataframe'])\n",
    "            it['markdown'] = md\n",
    "            texts.append(md)\n",
    "            text_idx.append(i)\n",
    "        elif typ == 'image':\n",
    "            # load and convert to data URI\n",
    "            with open(it['image_path'], 'rb') as f:\n",
    "                img_bytes = f.read()\n",
    "            # enforce max size 5MB\n",
    "            if len(img_bytes) > 5 * 1024 * 1024:\n",
    "                # downscale\n",
    "                im = Image.open(io.BytesIO(img_bytes))\n",
    "                im.thumbnail((1600, 1600))\n",
    "                buf = io.BytesIO()\n",
    "                im.save(buf, format='PNG')\n",
    "                img_bytes = buf.getvalue()\n",
    "            data_uri = _to_data_uri(img_bytes, 'png')\n",
    "            images.append(data_uri)\n",
    "            image_idx.append(i)\n",
    "\n",
    "    # Batch embed texts\n",
    "    if texts:\n",
    "        kwargs = {\n",
    "            'model': model,\n",
    "            'input_type': 'search_document',\n",
    "            'texts': texts,\n",
    "        }\n",
    "        if output_dim is not None:\n",
    "            kwargs['output_dimension'] = output_dim\n",
    "        resp = co.embed(**kwargs)\n",
    "        vecs = np.array(resp.embeddings.float, dtype=np.float32)\n",
    "        for idx, vec in zip(text_idx, vecs):\n",
    "            items[idx]['embedding'] = vec.tolist()\n",
    "\n",
    "    # Embed images (one at a time per API constraint)\n",
    "    for idx, data_uri in zip(image_idx, images):\n",
    "        kwargs = {\n",
    "            'model': model,\n",
    "            'input_type': 'image',\n",
    "            'images': [data_uri]\n",
    "        }\n",
    "        if output_dim is not None:\n",
    "            kwargs['output_dimension'] = output_dim\n",
    "        resp = co.embed(**kwargs)\n",
    "        vec = np.array(resp.embeddings.float[0], dtype=np.float32)\n",
    "        items[idx]['embedding'] = vec.tolist()\n",
    "\n",
    "    return items\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03f6d3d7",
   "metadata": {},
   "source": [
    "## Store in FAISS (cosine)\n",
    "We normalize vectors and store them in a single FAISS index so **text, tables, and images** are retrievable together. The `docstore` holds the original JSON per vector id."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b9e0c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MixedEmbeddingStore:\n",
    "    def __init__(self, dim: int):\n",
    "        self.dim = dim\n",
    "        self.index = faiss.IndexFlatIP(dim)\n",
    "        self.docstore = InMemoryDocstore({})\n",
    "        self.ids = []\n",
    "\n",
    "    def add(self, items: List[Dict[str, Any]]):\n",
    "        vecs = []\n",
    "        new_ids = []\n",
    "        for it in items:\n",
    "            emb = it.get('embedding')\n",
    "            if not emb:\n",
    "                continue\n",
    "            v = np.array(emb, dtype=np.float32)\n",
    "            v = v / (np.linalg.norm(v) + 1e-12)\n",
    "            vecs.append(v)\n",
    "            new_id = f\"doc-{len(self.ids) + len(new_ids)}\"\n",
    "            new_ids.append(new_id)\n",
    "            # wrap as LangChain Document for compatibility\n",
    "            meta = it.get('metadata', {}).copy()\n",
    "            meta.update({'type': it['type'], 'title': it.get('title', '')})\n",
    "            page_meta = {'page': meta.get('page'), 'start_page': meta.get('start_page'), 'end_page': meta.get('end_page')}\n",
    "            content = it.get('content') or it.get('markdown') or it.get('image_path') or ''\n",
    "            self.docstore.mset({new_id: Document(page_content=content, metadata=meta)})\n",
    "        if vecs:\n",
    "            mat = np.vstack(vecs)\n",
    "            self.index.add(mat)\n",
    "            self.ids.extend(new_ids)\n",
    "\n",
    "    def search(self, query_vec: np.ndarray, top_k: int = 5) -> List[Tuple[str, float]]:\n",
    "        q = query_vec.astype(np.float32)\n",
    "        q = q / (np.linalg.norm(q) + 1e-12)\n",
    "        D, I = self.index.search(q.reshape(1, -1), top_k)\n",
    "        hits = []\n",
    "        for d, i in zip(D[0], I[0]):\n",
    "            if i == -1:\n",
    "                continue\n",
    "            hits.append((self.ids[i], float(d)))\n",
    "        return hits\n",
    "\n",
    "    def get(self, _id: str) -> Document:\n",
    "        return self.docstore.search(_id)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6171e7a",
   "metadata": {},
   "source": [
    "## RAG Query\n",
    "1. Embed the query with `input_type='search_query'`.\n",
    "2. Retrieve top-k from FAISS.\n",
    "3. Send *only retrieved contexts* to Azure OpenAI with a strict grounding instruction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b104f9d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_query(query: str, co: cohere.ClientV2, model: str = EMBED_MODEL, output_dim: int = EMBED_OUTPUT_DIM) -> np.ndarray:\n",
    "    kwargs = {\n",
    "        'model': model,\n",
    "        'input_type': 'search_query',\n",
    "        'texts': [query]\n",
    "    }\n",
    "    if output_dim is not None:\n",
    "        kwargs['output_dimension'] = output_dim\n",
    "    resp = co.embed(**kwargs)\n",
    "    return np.array(resp.embeddings.float[0], dtype=np.float32)\n",
    "\n",
    "def answer_with_azure_openai(question: str, hits: List[Tuple[str, float]], store: MixedEmbeddingStore, llm: AzureChatOpenAI) -> Dict[str, Any]:\n",
    "    ctxs = []\n",
    "    for _id, score in hits:\n",
    "        doc = store.get(_id)\n",
    "        meta = doc.metadata\n",
    "        source = meta.get('source', 'unknown')\n",
    "        page = meta.get('page', meta.get('start_page'))\n",
    "        title = meta.get('title', '')\n",
    "        ctxs.append(f\"[type={meta.get('type')}, title={title}, source={source}, page={page}]\\n{doc.page_content}\")\n",
    "    system_msg = (\n",
    "        \"You are a precise assistant. Answer ONLY using the provided context snippets. \"\n",
    "        \"Cite the source filename and page(s) explicitly. If the answer isn't in the snippets, say you don't have it.\"\n",
    "    )\n",
    "    prompt = (\n",
    "        f\"Question: {question}\\n\\n\"\n",
    "        f\"Context:\\n\" + \"\\n\\n\".join(ctxs[:5])\n",
    "    )\n",
    "    resp = llm.invoke([{\"role\": \"system\", \"content\": system_msg}, {\"role\": \"user\", \"content\": prompt}])\n",
    "    return {\"answer\": resp.content, \"contexts\": ctxs}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb5eff92",
   "metadata": {},
   "source": [
    "## Orchestrator Functions\n",
    "These wrap each stage so you can run and validate step-by-step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "37425895",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'MixedEmbeddingStore' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[36]\u001b[39m\u001b[32m, line 15\u001b[39m\n\u001b[32m     12\u001b[39m     items = embed_items_with_cohere(items, co)\n\u001b[32m     13\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m items\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mbuild_store\u001b[39m(items: List[Dict[\u001b[38;5;28mstr\u001b[39m, Any]]) -> \u001b[43mMixedEmbeddingStore\u001b[49m:\n\u001b[32m     16\u001b[39m     \u001b[38;5;66;03m# Detect embedding dimension from first vector\u001b[39;00m\n\u001b[32m     17\u001b[39m     first = \u001b[38;5;28mnext\u001b[39m((it \u001b[38;5;28;01mfor\u001b[39;00m it \u001b[38;5;129;01min\u001b[39;00m items \u001b[38;5;28;01mif\u001b[39;00m it.get(\u001b[33m'\u001b[39m\u001b[33membedding\u001b[39m\u001b[33m'\u001b[39m)), \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m     18\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m first \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[33m\"\u001b[39m\u001b[33mNo embeddings produced.\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[31mNameError\u001b[39m: name 'MixedEmbeddingStore' is not defined"
     ]
    }
   ],
   "source": [
    "def run_extraction(pdf_path: str) -> Dict[str, List[Dict[str, Any]]]:\n",
    "    text_sections = extract_text_sections_by_headings(pdf_path)\n",
    "    images = extract_images(pdf_path)\n",
    "    tables = extract_tables(pdf_path)\n",
    "    return {\"text_sections\": text_sections, \"images\": images, \"tables\": tables}\n",
    "\n",
    "def run_embeddings(extracted: Dict[str, List[Dict[str, Any]]]) -> List[Dict[str, Any]]:\n",
    "    items = []\n",
    "    items.extend(extracted.get('text_sections', []))\n",
    "    items.extend(extracted.get('images', []))\n",
    "    items.extend(extracted.get('tables', []))\n",
    "    items = embed_items_with_cohere(items, co)\n",
    "    return items\n",
    "\n",
    "def build_store(items: List[Dict[str, Any]]) -> MixedEmbeddingStore:\n",
    "    # Detect embedding dimension from first vector\n",
    "    first = next((it for it in items if it.get('embedding')), None)\n",
    "    assert first is not None, \"No embeddings produced.\"\n",
    "    dim = len(first['embedding'])\n",
    "    store = MixedEmbeddingStore(dim)\n",
    "    store.add(items)\n",
    "    return store\n",
    "\n",
    "def query_pipeline(question: str, store: MixedEmbeddingStore, top_k: int = 5) -> Dict[str, Any]:\n",
    "    qv = embed_query(question, co)\n",
    "    hits = store.search(qv, top_k=top_k)\n",
    "    return answer_with_azure_openai(question, hits, store, llm)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "550338bf",
   "metadata": {},
   "source": [
    "## Verification Helpers\n",
    "Show JSON-like outputs in the specified format so you can confirm *type*, *title*, *embedding*, *metadata* at each step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5ead199",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preview_item_json(it: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    meta = it.get('metadata', {}).copy()\n",
    "    # keep only key fields\n",
    "    out = {\n",
    "        'type': it.get('type'),\n",
    "        'title': it.get('title'),\n",
    "        'embedding': (it.get('embedding')[:3] if it.get('embedding') else None),\n",
    "        'metadata': meta\n",
    "    }\n",
    "    # fill page convenience\n",
    "    if 'start_page' in meta and 'end_page' in meta and 'page' not in meta:\n",
    "        out['metadata']['page'] = meta['start_page'] if meta['start_page'] == meta['end_page'] else f\"{meta['start_page']}-{meta['end_page']}\"\n",
    "    return out\n",
    "\n",
    "def preview_many(items: List[Dict[str, Any]], n: int = 3):\n",
    "    for it in items[:n]:\n",
    "        print(json.dumps(preview_item_json(it), indent=2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dd26471",
   "metadata": {},
   "source": [
    "## Example Run (Step-by-Step)\n",
    "Uncomment and point `PDF_PATH` to your file, then run the cells one by one to verify each stage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "846675bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\INNIMK\\AppData\\Local\\anaconda3\\envs\\multimodal_rag_env\\Lib\\site-packages\\camelot\\parsers\\base.py:238: UserWarning: No tables found in table area (14.525267115945432, 196.75338878994157, 473.63975999999997, 401.2350332724647)\n",
      "  cols, rows, v_s, h_s = self._generate_columns_and_rows(bbox, user_cols)\n",
      "c:\\Users\\INNIMK\\AppData\\Local\\anaconda3\\envs\\multimodal_rag_env\\Lib\\site-packages\\camelot\\parsers\\base.py:238: UserWarning: No tables found in table area (17.74046214523443, 114.4132, 274.74224, 228.37684254333254)\n",
      "  cols, rows, v_s, h_s = self._generate_columns_and_rows(bbox, user_cols)\n",
      "c:\\Users\\INNIMK\\AppData\\Local\\anaconda3\\envs\\multimodal_rag_env\\Lib\\site-packages\\camelot\\parsers\\base.py:238: UserWarning: No tables found in table area (14.525267115945432, 196.75338878994157, 88.13507419174674, 400.09204071947863)\n",
      "  cols, rows, v_s, h_s = self._generate_columns_and_rows(bbox, user_cols)\n",
      "c:\\Users\\INNIMK\\AppData\\Local\\anaconda3\\envs\\multimodal_rag_env\\Lib\\site-packages\\camelot\\parsers\\base.py:238: UserWarning: No tables found in table area (14.525267115945432, 196.75338878994157, 88.13507419174674, 399.2165001111572)\n",
      "  cols, rows, v_s, h_s = self._generate_columns_and_rows(bbox, user_cols)\n",
      "c:\\Users\\INNIMK\\AppData\\Local\\anaconda3\\envs\\multimodal_rag_env\\Lib\\site-packages\\camelot\\parsers\\base.py:238: UserWarning: No tables found in table area (14.525267115945432, 196.75338878994157, 88.13507419174674, 399.65077136432853)\n",
      "  cols, rows, v_s, h_s = self._generate_columns_and_rows(bbox, user_cols)\n",
      "c:\\Users\\INNIMK\\AppData\\Local\\anaconda3\\envs\\multimodal_rag_env\\Lib\\site-packages\\camelot\\parsers\\base.py:238: UserWarning: No tables found in table area (14.525267115945432, 196.75338878994157, 88.13507419174674, 400.774228910773)\n",
      "  cols, rows, v_s, h_s = self._generate_columns_and_rows(bbox, user_cols)\n",
      "c:\\Users\\INNIMK\\AppData\\Local\\anaconda3\\envs\\multimodal_rag_env\\Lib\\site-packages\\camelot\\parsers\\base.py:238: UserWarning: No tables found in table area (102.61, 479.78319999999997, 577.7493599999998, 723.4529422851641)\n",
      "  cols, rows, v_s, h_s = self._generate_columns_and_rows(bbox, user_cols)\n",
      "c:\\Users\\INNIMK\\AppData\\Local\\anaconda3\\envs\\multimodal_rag_env\\Lib\\site-packages\\camelot\\parsers\\base.py:238: UserWarning: No tables found in table area (14.525267115945432, 196.75338878994157, 88.13507419174674, 400.36111193189237)\n",
      "  cols, rows, v_s, h_s = self._generate_columns_and_rows(bbox, user_cols)\n",
      "c:\\Users\\INNIMK\\AppData\\Local\\anaconda3\\envs\\multimodal_rag_env\\Lib\\site-packages\\camelot\\parsers\\base.py:238: UserWarning: No tables found in table area (14.525267115945432, 196.75338878994157, 88.13507419174674, 400.46066107157696)\n",
      "  cols, rows, v_s, h_s = self._generate_columns_and_rows(bbox, user_cols)\n",
      "c:\\Users\\INNIMK\\AppData\\Local\\anaconda3\\envs\\multimodal_rag_env\\Lib\\site-packages\\camelot\\parsers\\base.py:238: UserWarning: No tables found in table area (14.525267115945432, 196.75338878994157, 88.13507419174674, 399.30526324642074)\n",
      "  cols, rows, v_s, h_s = self._generate_columns_and_rows(bbox, user_cols)\n",
      "c:\\Users\\INNIMK\\AppData\\Local\\anaconda3\\envs\\multimodal_rag_env\\Lib\\site-packages\\camelot\\parsers\\base.py:238: UserWarning: No tables found in table area (14.525267115945432, 196.75338878994157, 88.13507419174674, 400.8799057891619)\n",
      "  cols, rows, v_s, h_s = self._generate_columns_and_rows(bbox, user_cols)\n",
      "c:\\Users\\INNIMK\\AppData\\Local\\anaconda3\\envs\\multimodal_rag_env\\Lib\\site-packages\\camelot\\parsers\\base.py:238: UserWarning: No tables found in table area (14.525267115945432, 196.75338878994157, 88.13507419174674, 398.80528162769195)\n",
      "  cols, rows, v_s, h_s = self._generate_columns_and_rows(bbox, user_cols)\n",
      "c:\\Users\\INNIMK\\AppData\\Local\\anaconda3\\envs\\multimodal_rag_env\\Lib\\site-packages\\camelot\\parsers\\base.py:238: UserWarning: No tables found in table area (14.525267115945432, 196.75338878994157, 88.13507419174674, 398.3711860671933)\n",
      "  cols, rows, v_s, h_s = self._generate_columns_and_rows(bbox, user_cols)\n",
      "c:\\Users\\INNIMK\\AppData\\Local\\anaconda3\\envs\\multimodal_rag_env\\Lib\\site-packages\\camelot\\parsers\\base.py:238: UserWarning: No tables found in table area (14.525267115945432, 196.75338878994157, 107.38624, 399.3577862495038)\n",
      "  cols, rows, v_s, h_s = self._generate_columns_and_rows(bbox, user_cols)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text sections: 5389\n",
      "Images: 7\n",
      "Tables: 310\n"
     ]
    }
   ],
   "source": [
    "PDF_PATH = r\"\"\n",
    "\n",
    "# # 1) Extraction\n",
    "extracted = run_extraction(PDF_PATH)\n",
    "print(\"Text sections:\", len(extracted['text_sections']))\n",
    "print(\"Images:\", len(extracted['images']))\n",
    "print(\"Tables:\", len(extracted['tables']))\n",
    "\n",
    "# # 2) Embeddings\n",
    "items = run_embeddings(extracted)\n",
    "preview_many(items, n=5)\n",
    "\n",
    "# # 3) Vector store\n",
    "store = build_store(items)\n",
    "\n",
    "# # 4) Query\n",
    "question = \"What are the annual sales figures and where are they reported?\"\n",
    "result = query_pipeline(question, store, top_k=5)\n",
    "print(result['answer'])\n",
    "print(\"\\nContexts used:\\n\", \"\\n---\\n\".join(result['contexts']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "974e2081",
   "metadata": {},
   "source": [
    "### Optional: Persist/Load the Store\n",
    "You can persist embeddings and metadata to disk and reload to avoid recomputing for the same PDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b779d227",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_items_json(items: List[Dict[str, Any]], json_path: str):\n",
    "    with open(json_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(items, f, ensure_ascii=False)\n",
    "\n",
    "def load_items_json(json_path: str) -> List[Dict[str, Any]]:\n",
    "    with open(json_path, 'r', encoding='utf-8') as f:\n",
    "        return json.load(f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f913d7b2",
   "metadata": {},
   "source": [
    "## Save Extracted Chunks, Images, and Tables to JSON\n",
    "This function organizes the extracted text sections, images, and tables into a single JSON file with metadata so you can persist the results or inspect them easily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8ef9ddd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_extracted_to_json(text_sections: List[Dict[str, Any]], images: List[Dict[str, Any]], tables: List[Dict[str, Any]], json_path: str) -> str:\n",
    "    \"\"\"\n",
    "    Save the extracted text sections, images, and tables into a JSON file.\n",
    "\n",
    "    DataFrames in tables are converted into simple serializable structures with\n",
    "    headers and rows so that json.dump does not fail.\n",
    "\n",
    "    Args:\n",
    "        text_sections: List of text section dicts (each has type='text', title, content, metadata)\n",
    "        images: List of image dicts (type='image', title, image_path/data_uri, metadata)\n",
    "        tables: List of table dicts (each may contain a pandas DataFrame under 'dataframe')\n",
    "        json_path: Path to output JSON file\n",
    "\n",
    "    Returns:\n",
    "        The path to the saved JSON file.\n",
    "    \"\"\"\n",
    "    import json\n",
    "    serializable_tables = []\n",
    "    for tbl in tables:\n",
    "        # Convert pandas DataFrame into a serializable structure\n",
    "        df = tbl.get('dataframe')\n",
    "        if df is not None and hasattr(df, 'values'):\n",
    "            headers = list(df.columns)\n",
    "            rows = df.values.tolist()\n",
    "            serializable_tables.append({\n",
    "                'type': 'table',\n",
    "                'title': tbl.get('title'),\n",
    "                'headers': headers,\n",
    "                'rows': rows,\n",
    "                'metadata': tbl.get('metadata', {})\n",
    "            })\n",
    "        else:\n",
    "            # If no DataFrame is present, assume already serializable\n",
    "            serializable_tables.append(tbl)\n",
    "    data = {\n",
    "        'text_sections': text_sections,\n",
    "        'images': images,\n",
    "        'tables': serializable_tables\n",
    "    }\n",
    "    with open(json_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(data, f, ensure_ascii=False, indent=2)\n",
    "    return json_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ce0f6807",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted results saved to extracted_results.json\n"
     ]
    }
   ],
   "source": [
    "output_json_path = \"extracted_results.json\"\n",
    "save_extracted_to_json(\n",
    "    extracted[\"text_sections\"],\n",
    "    extracted[\"images\"],\n",
    "    extracted[\"tables\"],\n",
    "    output_json_path\n",
    ")\n",
    "print(f\"Extracted results saved to {output_json_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3161d47a",
   "metadata": {},
   "source": [
    "## Azure AI Search Integration\n",
    "This section defines functions to create an Azure AI Search index suitable for storing mixed modalities (text, images, tables) and upload your documents with embeddings. The index schema includes fields for the document type, titles, content, page numbers, and vector embeddings. It follows guidance from Microsoft's documentation so that the index can be used with vector search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7cb7ee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_azure_ai_search_index(service_endpoint: str, admin_key: str, index_name: str, vector_dim: int):\n",
    "    \"\"\"\n",
    "    Create (or recreate) an Azure AI Search index with fields suitable for mixed-modality retrieval.\n",
    "\n",
    "    The index contains:\n",
    "      - id (string, key)\n",
    "      - doc_type (filterable string to distinguish text/table/image)\n",
    "      - title (searchable string)\n",
    "      - content (searchable string) for text and tables\n",
    "      - embedding (vector field for semantic search)\n",
    "      - page_start, page_end (filterable ints)\n",
    "      - image_path (string) optional for images\n",
    "\n",
    "    A HNSW vector search configuration is used with cosine similarity.\n",
    "    \"\"\"\n",
    "    from azure.core.credentials import AzureKeyCredential\n",
    "    from azure.search.documents.indexes import SearchIndexClient\n",
    "    from azure.search.documents.indexes.models import (\n",
    "        SearchIndex, SimpleField, SearchableField, VectorField, VectorSearch,\n",
    "        VectorSearchAlgorithmConfiguration, HnswParameters\n",
    "    )\n",
    "\n",
    "    credential = AzureKeyCredential(admin_key)\n",
    "    index_client = SearchIndexClient(endpoint=service_endpoint, credential=credential)\n",
    "\n",
    "    fields = [\n",
    "        SimpleField(name=\"id\", type=\"Edm.String\", key=True),\n",
    "        SimpleField(name=\"doc_type\", type=\"Edm.String\", filterable=True, facetable=True),\n",
    "        SearchableField(name=\"title\", type=\"Edm.String\", analyzer_name=\"en.lucene\"),\n",
    "        SearchableField(name=\"content\", type=\"Edm.String\", analyzer_name=\"en.lucene\"),\n",
    "        VectorField(name=\"embedding\", vector_dimensions=vector_dim, vector_search_configuration=\"my-vector-config\"),\n",
    "        SimpleField(name=\"page_start\", type=\"Edm.Int32\", filterable=True),\n",
    "        SimpleField(name=\"page_end\", type=\"Edm.Int32\", filterable=True),\n",
    "        SearchableField(name=\"image_path\", type=\"Edm.String\", analyzer_name=None)\n",
    "    ]\n",
    "\n",
    "    vector_search = VectorSearch(\n",
    "        algorithm_configurations=[\n",
    "            VectorSearchAlgorithmConfiguration(\n",
    "                name=\"my-vector-config\",\n",
    "                kind=\"hnsw\",\n",
    "                hnsw_parameters=HnswParameters(\n",
    "                    m=4,\n",
    "                    ef_construction=400,\n",
    "                    ef_search=500,\n",
    "                    metric=\"cosine\"\n",
    "                )\n",
    "            )\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    index = SearchIndex(name=index_name, fields=fields, vector_search=vector_search)\n",
    "    try:\n",
    "        index_client.delete_index(index_name)\n",
    "    except Exception:\n",
    "        pass\n",
    "    created = index_client.create_or_update_index(index)\n",
    "    return created\n",
    "\n",
    "\n",
    "def prepare_documents_for_azure(items: List[Dict[str, Any]]) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Prepare a list of documents from extracted items for uploading to Azure AI Search.\n",
    "\n",
    "    Each returned dict contains the fields defined in the index schema. Note that image documents\n",
    "    store the image path instead of content, while text and table documents use the 'content' key.\n",
    "    \"\"\"\n",
    "    import uuid\n",
    "    docs = []\n",
    "    for i, it in enumerate(items):\n",
    "        doc = {\n",
    "            'id': str(uuid.uuid4()),\n",
    "            'doc_type': it['type'],\n",
    "            'title': it.get('title', ''),\n",
    "            'content': it.get('content', it.get('table_md', '')) if it['type'] != 'image' else '',\n",
    "            'embedding': it.get('embedding').tolist() if it.get('embedding') is not None else None,\n",
    "            'page_start': it.get('metadata', {}).get('page_start'),\n",
    "            'page_end': it.get('metadata', {}).get('page_end'),\n",
    "            'image_path': it.get('path', '') if it['type'] == 'image' else ''\n",
    "        }\n",
    "        docs.append(doc)\n",
    "    return docs\n",
    "\n",
    "\n",
    "def upload_documents_to_azure(service_endpoint: str, admin_key: str, index_name: str, documents: List[Dict[str, Any]]):\n",
    "    \"\"\"\n",
    "    Upload a batch of documents to Azure AI Search. Documents should already include vector embeddings.\n",
    "\n",
    "    Returns the result of the upload operation.\n",
    "    \"\"\"\n",
    "    from azure.core.credentials import AzureKeyCredential\n",
    "    from azure.search.documents import SearchClient\n",
    "\n",
    "    search_client = SearchClient(endpoint=service_endpoint, index_name=index_name, credential=AzureKeyCredential(admin_key))\n",
    "    # Upload in batches\n",
    "    batch_size = 1000\n",
    "    results = []\n",
    "    for i in range(0, len(documents), batch_size):\n",
    "        batch = documents[i:i+batch_size]\n",
    "        result = search_client.upload_documents(documents=batch)\n",
    "        results.append(result)\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42896482",
   "metadata": {},
   "source": [
    "## Querying Azure AI Search with LLM\n",
    "This function demonstrates how to query the Azure AI Search index using a natural language question. It embeds the query, performs a vector search against the index, and then uses an Azure OpenAI chat model to generate an answer grounded in the retrieved contexts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9781196d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_with_llm(query: str, co: cohere.ClientV2, search_client, top_k: int = 5) -> str:\n",
    "    \"\"\"\n",
    "    Answer a question using the Azure AI Search index and an LLM.\n",
    "\n",
    "    Steps:\n",
    "      1. Embed the query with Cohere using input_type='search_query'.\n",
    "      2. Perform a vector search on the index to retrieve the most relevant documents.\n",
    "      3. Combine the retrieved contexts and send them to the Azure OpenAI chat completion API.\n",
    "\n",
    "    Args:\n",
    "        query: The natural language question.\n",
    "        co: A Cohere Client used to embed the query.\n",
    "        search_client: An instance of azure.search.documents.SearchClient connected to the target index.\n",
    "        top_k: How many documents to retrieve.\n",
    "\n",
    "    Returns:\n",
    "        The LLM's answer as a string.\n",
    "    \"\"\"\n",
    "    # Embed the query\n",
    "    query_embedding = embed_query(query, co)\n",
    "    # Perform vector search\n",
    "    results = search_client.search(\n",
    "        search_text=\"\",\n",
    "        vector=query_embedding.tolist(),\n",
    "        top_k=top_k,\n",
    "        vector_fields=\"embedding\",\n",
    "        select=[\"title\", \"content\", \"doc_type\", \"page_start\", \"page_end\"]\n",
    "    )\n",
    "    contexts = []\n",
    "    for r in results:\n",
    "        # Some documents may have empty content (images). Skip those for context.\n",
    "        if r.get('content'):\n",
    "            contexts.append(r['content'])\n",
    "    # Build a prompt for the chat model\n",
    "    joined_contexts = \"\".join(contexts)\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant that answers questions using provided document contexts. If the answer is not contained in the contexts, respond that you don't know.\"},\n",
    "        {\"role\": \"user\", \"content\": f\"Context:{joined_contexts} Question: {query}\"}\n",
    "    ]\n",
    "    # Ask the LLM\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=LLM_MODEL,\n",
    "        messages=messages,\n",
    "        temperature=0.0\n",
    "    )\n",
    "    return response['choices'][0]['message']['content']\n"
   ]
  }
 ],
 "metadata": {
  "authors": [
   {
    "created": "2025-08-08T07:03:16.631419",
    "name": "ChatGPT Assistant"
   }
  ],
  "kernelspec": {
   "display_name": "multimodal_rag_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
